#************************************************************************************
#* References the Documentation for all the settings below here:                   **
#* https://github.com/jparkerweb/web-augmented-generation?tab=readme-ov-file#setup **
#************************************************************************************


######################
## General Settings ##
######################
NUM_URLS=10                                                           # Number of URLs to fetch
SEARXNG_URL=https://searx.be/                                         # URL of the SearXNG server
SEARXNG_URL_EXTRA_PARAMETER="key=optional_auth_key_here&language=en"  # Extra parameter for SearXNG URL
SEARXNG_FORMAT=html                                                   # Format for SearXNG results (html or json)
FETCH_TIMEOUT_MS=5000                                                 # Timeout for fetching URLs
DISABLE_SSL_VALIDATION=true                                           # Whether to disable SSL validation




##################
## LLM Settings ##
##################

LLM_STREAM_RESPONSE=true                             # Whether to stream the LLM response

#------------------
#- Ollama Local --
#------------------
LLM_BASE_URL=http://localhost:11434/v1               # Base URL for the LLM API (OpenAI format)
LLM_API_KEY=ollama!!!                                # API key for the LLM (use 'ollama' for Ollama)
LLM_MODEL=llama3.2:1b                                # Model to use with the LLM API

#----------------
#- together.ai --
#----------------
# LLM_BASE_URL=https://api.together.xyz/v1           # Base URL for the LLM API (OpenAI format)
# LLM_API_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx  # API key for the LLM (use 'ollama' for Ollama)
# LLM_MODEL=meta-llama/Llama-3.2-3B-Instruct-Turbo   # Model to use with the LLM API

#--------------
#- llama.cpp --
#--------------
# LLM_BASE_URL=http://localhost:8080/v1              # Base URL for the LLM API (OpenAI format)
# LLM_API_KEY=not-needed                             # API key for the LLM (use 'ollama' for Ollama)
# LLM_MODEL=not-needed                               # Model to use with the LLM API




####################################
## Scrapted Page Content Settings ##
####################################

#-----------------------
#- Semantic Chunking? --
#-------------------------------------------------------------------------------------
#- If you want to use semantic chunking, set CHUNK_CONTENT to true (default).
#- Chunked content takes longer to process, but can improve the quality of the answer
#- and reduce the number of tokens used in the LLM query.
#-------------------------------------------------------------------------------------
CHUNK_CONTENT=true
## The following parameters are only used by the `chunk-match` library (if CHUNK_CONTENT is set to true)
CHUNK_CONTENT_MAX_RESULTS=10
CHUNK_CONTENT_MIN_SIMILARITY=0.475
CHUNK_CONTENT_MAX_TOKEN_SIZE=500
CHUNK_CONTENT_SIMILARITY_THRESHOLD=0.5
CHUNK_CONTENT_DYNAMIC_THRESHOLD_LOWER_BOUND=0.4
CHUNK_CONTENT_DYNAMIC_THRESHOLD_UPPER_BOUND=0.8
CHUNK_CONTENT_NUM_SIMILARITY_SENTENCES_LOOKAHEAD=3
CHUNK_CONTENT_COMBINE_CHUNKS=true
CHUNK_CONTENT_COMBINE_CHUNKS_SIMILARITY_THRESHOLD=0.6
CHUNK_CONTENT_ONNX_EMBEDDING_MODEL="Xenova/all-MiniLM-L6-v2"
CHUNK_CONTENT_DTYPE="q8"

#----------------------------------------
#- Trimmed Raw Web Page Content Length --
#----------------------------------------
# If `CHUNK_CONTENT` above is set to `false`, raw page content will be truncated
# and sent to the LLM in hopes of answering the question. The value below
# determines the maximum length of the raw page content to send to the LLM.
WEB_PAGE_CONTENT_MAX_LENGTH=1000
